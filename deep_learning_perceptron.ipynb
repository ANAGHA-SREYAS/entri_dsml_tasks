{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjR6K/5u0vxGEQFztgF1SH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANAGHA-SREYAS/entri_dsml_tasks/blob/main/deep_learning_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING A PERCEPTRON MODEL"
      ],
      "metadata": {
        "id": "h6FpQfL_4-e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load iris data set\n",
        "Scikit has a package called sklearn.datasets that contains sample datasets for quick testing. Inside this package is the Iris dataset. This data set contains 3 classes of 50 instances each, where each class refers to a type of Iris plant: Iris-Setosa, Iris-Versicolour, and Iris-Virginica. Each instance contains four attributes: sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "We will train our model using only petal length and petal width. Moreover, since a perceptron is limited to binary output, our perceptron will only detect whether the Iris plant is an Iris-Setosa or not"
      ],
      "metadata": {
        "id": "y26JPmis4mD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsH7Y2PL4Izi",
        "outputId": "eff7bf13-725b-4188-d646-2df401b82261"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4, 0.2],\n",
              "       [1.4, 0.2],\n",
              "       [1.3, 0.2],\n",
              "       [1.5, 0.2],\n",
              "       [1.4, 0.2],\n",
              "       [1.7, 0.4],\n",
              "       [1.4, 0.3],\n",
              "       [1.5, 0.2],\n",
              "       [1.4, 0.2],\n",
              "       [1.5, 0.1],\n",
              "       [1.5, 0.2],\n",
              "       [1.6, 0.2],\n",
              "       [1.4, 0.1],\n",
              "       [1.1, 0.1],\n",
              "       [1.2, 0.2],\n",
              "       [1.5, 0.4],\n",
              "       [1.3, 0.4],\n",
              "       [1.4, 0.3],\n",
              "       [1.7, 0.3],\n",
              "       [1.5, 0.3],\n",
              "       [1.7, 0.2],\n",
              "       [1.5, 0.4],\n",
              "       [1. , 0.2],\n",
              "       [1.7, 0.5],\n",
              "       [1.9, 0.2],\n",
              "       [1.6, 0.2],\n",
              "       [1.6, 0.4],\n",
              "       [1.5, 0.2],\n",
              "       [1.4, 0.2],\n",
              "       [1.6, 0.2],\n",
              "       [1.6, 0.2],\n",
              "       [1.5, 0.4],\n",
              "       [1.5, 0.1],\n",
              "       [1.4, 0.2],\n",
              "       [1.5, 0.2],\n",
              "       [1.2, 0.2],\n",
              "       [1.3, 0.2],\n",
              "       [1.4, 0.1],\n",
              "       [1.3, 0.2],\n",
              "       [1.5, 0.2],\n",
              "       [1.3, 0.3],\n",
              "       [1.3, 0.3],\n",
              "       [1.3, 0.2],\n",
              "       [1.6, 0.6],\n",
              "       [1.9, 0.4],\n",
              "       [1.4, 0.3],\n",
              "       [1.6, 0.2],\n",
              "       [1.4, 0.2],\n",
              "       [1.5, 0.2],\n",
              "       [1.4, 0.2],\n",
              "       [4.7, 1.4],\n",
              "       [4.5, 1.5],\n",
              "       [4.9, 1.5],\n",
              "       [4. , 1.3],\n",
              "       [4.6, 1.5],\n",
              "       [4.5, 1.3],\n",
              "       [4.7, 1.6],\n",
              "       [3.3, 1. ],\n",
              "       [4.6, 1.3],\n",
              "       [3.9, 1.4],\n",
              "       [3.5, 1. ],\n",
              "       [4.2, 1.5],\n",
              "       [4. , 1. ],\n",
              "       [4.7, 1.4],\n",
              "       [3.6, 1.3],\n",
              "       [4.4, 1.4],\n",
              "       [4.5, 1.5],\n",
              "       [4.1, 1. ],\n",
              "       [4.5, 1.5],\n",
              "       [3.9, 1.1],\n",
              "       [4.8, 1.8],\n",
              "       [4. , 1.3],\n",
              "       [4.9, 1.5],\n",
              "       [4.7, 1.2],\n",
              "       [4.3, 1.3],\n",
              "       [4.4, 1.4],\n",
              "       [4.8, 1.4],\n",
              "       [5. , 1.7],\n",
              "       [4.5, 1.5],\n",
              "       [3.5, 1. ],\n",
              "       [3.8, 1.1],\n",
              "       [3.7, 1. ],\n",
              "       [3.9, 1.2],\n",
              "       [5.1, 1.6],\n",
              "       [4.5, 1.5],\n",
              "       [4.5, 1.6],\n",
              "       [4.7, 1.5],\n",
              "       [4.4, 1.3],\n",
              "       [4.1, 1.3],\n",
              "       [4. , 1.3],\n",
              "       [4.4, 1.2],\n",
              "       [4.6, 1.4],\n",
              "       [4. , 1.2],\n",
              "       [3.3, 1. ],\n",
              "       [4.2, 1.3],\n",
              "       [4.2, 1.2],\n",
              "       [4.2, 1.3],\n",
              "       [4.3, 1.3],\n",
              "       [3. , 1.1],\n",
              "       [4.1, 1.3],\n",
              "       [6. , 2.5],\n",
              "       [5.1, 1.9],\n",
              "       [5.9, 2.1],\n",
              "       [5.6, 1.8],\n",
              "       [5.8, 2.2],\n",
              "       [6.6, 2.1],\n",
              "       [4.5, 1.7],\n",
              "       [6.3, 1.8],\n",
              "       [5.8, 1.8],\n",
              "       [6.1, 2.5],\n",
              "       [5.1, 2. ],\n",
              "       [5.3, 1.9],\n",
              "       [5.5, 2.1],\n",
              "       [5. , 2. ],\n",
              "       [5.1, 2.4],\n",
              "       [5.3, 2.3],\n",
              "       [5.5, 1.8],\n",
              "       [6.7, 2.2],\n",
              "       [6.9, 2.3],\n",
              "       [5. , 1.5],\n",
              "       [5.7, 2.3],\n",
              "       [4.9, 2. ],\n",
              "       [6.7, 2. ],\n",
              "       [4.9, 1.8],\n",
              "       [5.7, 2.1],\n",
              "       [6. , 1.8],\n",
              "       [4.8, 1.8],\n",
              "       [4.9, 1.8],\n",
              "       [5.6, 2.1],\n",
              "       [5.8, 1.6],\n",
              "       [6.1, 1.9],\n",
              "       [6.4, 2. ],\n",
              "       [5.6, 2.2],\n",
              "       [5.1, 1.5],\n",
              "       [5.6, 1.4],\n",
              "       [6.1, 2.3],\n",
              "       [5.6, 2.4],\n",
              "       [5.5, 1.8],\n",
              "       [4.8, 1.8],\n",
              "       [5.4, 2.1],\n",
              "       [5.6, 2.4],\n",
              "       [5.1, 2.3],\n",
              "       [5.1, 1.9],\n",
              "       [5.9, 2.3],\n",
              "       [5.7, 2.5],\n",
              "       [5.2, 2.3],\n",
              "       [5. , 1.9],\n",
              "       [5.2, 2. ],\n",
              "       [5.4, 2.3],\n",
              "       [5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris=load_iris()\n",
        "x=iris.data[:,(2,3)]\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=iris.target\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jcvz8To6Kjb",
        "outputId": "c1025dde-21ab-45ee-c3f3-a62e4267ef4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " list of all 150 instances of Iris plants labeled with the target values: 0 as Iris-Setosa, 1 as Iris-Versicolour, and 2 as Iris-Virginica"
      ],
      "metadata": {
        "id": "s8rHHdow6Z2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=iris.target==0\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcWc_fko6cWr",
        "outputId": "e3def653-4310-4880-9671-a32915ccd3a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=(iris.target==0).astype(int)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BogLoHgf6tPC",
        "outputId": "638b496f-ed33-42a6-98ff-8ba4a3754456"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the perceptron"
      ],
      "metadata": {
        "id": "icWnoLpz7Enc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron"
      ],
      "metadata": {
        "id": "4cSn3h8T7Ini"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the parameters with random values, then fit the 150 pairs of petal width and length instances to y. This will teach the perceptron to distinguish the Iris Setosa among the 150 instances."
      ],
      "metadata": {
        "id": "VpQz2nCJ7l1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "per_clf=Perceptron(random_state=42)\n",
        "per_clf.fit(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Dgu7f5c-7mp_",
        "outputId": "7c23a948-8919-4abb-c3a2-1b20d07166f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prediction"
      ],
      "metadata": {
        "id": "LSyE2LgY8G4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=per_clf.predict([[1.37,0.29]])\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6fpa7Bg8JKM",
        "outputId": "88ad0292-5aab-4d41-d62d-408149fa7a40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To see the weights and bias learned by the Perceptron, you can use the coef_ and intercept_ attributes respectively\n",
        "\n",
        "The coef_ attribute will give you the weights (coefficients) of the model, and the intercept_ attribute will give you the bias term"
      ],
      "metadata": {
        "id": "-1PNAB079kGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights=per_clf.coef_[0]\n",
        "bias=per_clf.intercept_[0]\n",
        "print(\"Model Weights:\", weights)\n",
        "print(\"Model Bias:\", bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikW_RLWH9llF",
        "outputId": "73fffb76-18a2-429a-fbac-e096829aa8c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Weights: [-1.4 -2.2]\n",
            "Model Bias: 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learnable parameters in a neural network refer to the weights and biases that the model learns during the training process. These parameters are adjusted iteratively to minimize the error (loss) on the training data, so that the model can make accurate predictions on new, unseen data. The equation for finding learnable parameters is quite simple\n",
        "\n",
        "For a single-layer perceptron:\n",
        "\n",
        "The single-layer perceptron has one layer of weights and one bias term. Suppose the input has n features (inputs) and there is one output neuron. Then, the total number of learnable parameters in a single-layer perceptron is given by:\n",
        "\n",
        "Total parameters = Number of input features (n) + 1 (for the bias term)"
      ],
      "metadata": {
        "id": "AA7WOsfS-fBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "total_params = np.prod(per_clf.coef_.shape) + len(per_clf.intercept_)\n",
        "print(\"Total learnable parameters:\", total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRLf9Oc-ers",
        "outputId": "b4a79651-4fa7-4baa-e575-f342338295b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total learnable parameters: 3\n"
          ]
        }
      ]
    }
  ]
}